---
title: "HW2"
output: html_document
date: "2025-10-24"
---
# LSA 예제3: mindscale 예제

```{r}
library(tidyverse)
library(lsa)
library(LSAfun)
library(wordcloud)
library(plotrix)
library(tm)

#X는 n*p 행렬 = UDV를 X^로 근사= UDV' (n*m)(m*m)(m*p)/ m은 토픽의 수
DC <- read.csv("C:/Users/USER/Downloads/News-wikipedia-DFE.csv")
dim(DC)
head(DC)

#TD생성
ctrl <- list(removeNumbers=TRUE, #숫자 없애기
             removePunctuation=TRUE,  #구둣점 없애기
             wordLengths=c(3, Inf), #3글자 미만  지우기
             stopwords=stopwords('en'))  #stopwords=c('바보','어디')->부정어, 조사, a/an/the와 같은

TD <- TermDocumentMatrix(Corpus(VectorSource(DC$newdescp)), control=ctrl)
c(nTerms(TD), nDocs(TD))

TD #1억 칸중  3만개만 사용
```

## wordcloud
```{r}
library(slam)
wrdfrq <- sort(row_sums(TD), decreasing=TRUE)
wrd1000 <- wrdfrq[1:1000]
wordcloud(names(wrd1000), wrd1000, min.freq=500)
```

## LSAspace 적합
```{r}
TDM <- as.matrix(TD[names(wrd1000),])
TDM <- lw_bintf(TDM) * gw_idf(TDM)  # TFIDF로 변환
Mlsa <- lsa(TDM, dims=30) 
c(dim(Mlsa$tk), length(Mlsa$sk), dim(Mlsa$dk)) 
TDMh <- as.textmatrix(Mlsa)  
```

## 토픽분석
```{r}
library(GPArotation)
#성분= 잠재변수
#성분의 해석을 쉽게 하는게 목적 => 계수들이 0이 많이 나오면 좋다
Urot <- Varimax(Mlsa$tk)$loadings #주성분 게수를 저장
colnames(Urot) <- paste0('topic', sprintf('%02d', 1:30))
round(Urot[1:5, 1:8], 2) #-0.03said+0.00ppl+.....

for(topic in 1:ncol(Urot)){
  cat('Topic ', topic, '\n')
  imp <- order(abs(Urot[,topic]), decreasing=TRUE)
  print(Urot[imp[1:5], topic])
}

Urot <- as_tibble(Urot, rownames='term')
tidy_lsa <- pivot_longer(Urot, 
                         cols=starts_with('topic'), 
                         names_to='topic', 
                         values_to='Uir')

# 계수절대값 크기순 5개 단어
top_terms <- tidy_lsa %>% 
  group_by(topic) %>% 
  top_n(5, abs(Uir)) %>% 
  ungroup() %>% 
  arrange(topic, -abs(Uir))
top_terms
#표가 나왔을때 뭘 봐야할지 알아야 한다

tail(top_terms)

top_terms %>% filter(grepl('topic0', topic)) %>%
  mutate(term = reorder(term, Uir)) %>%
  group_by(topic, term) %>%
  arrange(desc(Uir)) %>%
  ungroup() %>%
  mutate(term=factor(paste(term, topic, sep='__'), 
                     levels=rev(paste(term, topic, sep='__')))) %>%
  ggplot(aes(term, Uir, fill=as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  scale_x_discrete(labels = function(x) gsub('__.+$', '', x)) +
  labs(title='Top words in LSA topic 01-09', x=NULL, y=expression(Uir)) +
  facet_wrap(~ topic, ncol=3, scales='free')
#안의 계수가 음수가 안나오게

top_terms %>% filter(grepl('topic1', topic)) %>%
 mutate(term = reorder(term, Uir)) %>%
 group_by(topic, term) %>%
 arrange(desc(Uir)) %>%
 ungroup() %>%
 mutate(term=factor(paste(term, topic, sep='__'), 
                    levels=rev(paste(term, topic, sep='__')))) %>%
 ggplot(aes(term, Uir, fill=as.factor(topic))) +
 geom_col(show.legend = FALSE) +
 coord_flip() +
 scale_x_discrete(labels = function(x) gsub('__.+$', '', x)) +
 labs(title='Top words in LSA topic 10-19', x=NULL, y=expression(Uir)) +
 facet_wrap(~ topic, ncol=3, scales='free')


top_terms %>% filter(grepl('topic2', topic)|topic=='topic30') %>%
 mutate(term = reorder(term, Uir)) %>%
 group_by(topic, term) %>%
 arrange(desc(Uir)) %>%
 ungroup() %>%
 mutate(term=factor(paste(term, topic, sep='__'), 
                    levels=rev(paste(term, topic, sep='__')))) %>%
 ggplot(aes(term, Uir, fill=as.factor(topic))) +
 geom_col(show.legend = FALSE) +
 coord_flip() +
 scale_x_discrete(labels = function(x) gsub('__.+$', '', x)) +
 labs(title='Top words in LSA topic 20-29', x=NULL, y=expression(Uir)) +
 facet_wrap(~ topic, ncol=3, scales='free')
```

## term space: U = $tk 단어간 분석
```{r}
A <- TDM %*% Mlsa$dk 
# cosine(t(A)) : A0 <- normalize(A, 1); Rtrm <- A0 %*% t(A0) 와 동일
Rtrm <- lsa::cosine(t(A))   #코사인 유사도
heatmap(Rtrm[1:20, 1:20]) #1000*1000 행렬

# 유사단어: korea와 유사한 단어 5개
sort(Rtrm['korea',], decreasing=TRUE)[1:5]
LSAfun::Cosine('korea', 'missile', tvectors=TDMh)
LSAfun::multicos(c('hamas','israel','gaza'), tvectors=TDMh)
# 특정단어의 유사어 조회 및 시각화 
LSAfun::neighbors('korea', n=10,  tvectors=TDMh)

LSAfun::plot_neighbors(
  'korea',            # 조회어 
  n = 10,             # number of neighbors
  tvectors = TDMh,    # matrix space
  method   = 'MDS',   # PCA or MDS (차원축소)
  dims = 2)           # number of dimensions

# 특정단어와의 유사도가 lower와 upper 사이에 있는 단어 중 낮은 순 n개 추출
LSAfun::choose.target(
  'korea',      # 조회어
  lower = 0.3,  # lower cosine
  upper = 0.7,  # upper cosine
  n = 10,       # number of related words to get
  tvectors = TDMh)

# 단어간 유사도 2D/3D 시각화
x <- c('korea', 'north', 'south','china', 'seoul',
       'israel', 'gaza', 'afghanistan', 
       'russia', 'europe', 'london',
       'soldier', 'political')
plot_wordlist(x, tvectors=TDMh, dims=2)
```

## doc space: V=$dk 문서간 분석
```{r}
#doc space: 기사간 유사도 계산
B <- t(Mlsa$tk) %*% TDM
# cosine(B) : B0 <- normalize(B, 2); Rdoc <- t(B0) %*% B0 와 동일
Rdoc <- lsa::cosine(B)   
heatmap(Rdoc[1:30, 1:30])

# 유사문서: (1,18), (72,99)
str_sub(DC$newdescp[1], 1, 100)
str_sub(DC$newdescp[18], 1, 100)

#문서간 유사도
Rdoc[1,18] #상관계수와 유사도는 다르다

str_sub(DC$newdescp[72], 1, 100)
str_sub(DC$newdescp[77], 1, 100)
str_sub(DC$newdescp[99], 1, 100)
Rdoc[72,99]
Rdoc[77,99]
# 1번 문서와 유사한 문서 10개. 2935,2131
sort(Rdoc[1,], decreasing=TRUE)[1:10]
str_sub(DC$newdescp[2935], 1, 100)
str_sub(DC$newdescp[2131], 1, 100)

#회전한 U를 사용해도 최종 코사인유사도는 그대로
library(GPArotation)
Urot <- Varimax(Mlsa$tk)$loadings
Brot <- t(Urot) %*% TDM
Rdoc2 <- lsa::cosine(Brot)
heatmap(Rdoc2[1:30, 1:30])
sort(Rdoc2[1,], decreasing=TRUE)[1:10]
```

## 문서/ 문장간 유사도: 문장비교,표절검사, 키워드 채점

```{r}
# LSAfun::costring(x:벡터, y:벡터, tv, ..): 문장비교. 단어간, 문장간, 문서간, 문장-문서간 유사도
LSAfun::costring('south', 'korea', tvectors=TDMh)  # Cosine과 동일
LSAfun::costring('south korea talks to united states', 
                 'north korea talks to china', 
                 tvectors=TDMh) # 두 문장 유사도
LSAfun::costring('south korea talks to united states', 
                 'israel talks about gaza', 
                 tvectors=TDMh) # 두 문장 유사도
LSAfun::costring(DC$newdescp[77], DC$newdescp[99], tvectors=TDMh)
Rdoc[77,99]

#multicostring(x:문장, y:단어벡터, tv, ..): x내 문장과 y내 단어간 유사도. 키워드 채점
multicostring(x=DC$newdescp[77],
              y=c('missile', 'nuclear', 'china'), 
              tvectors=TDMh)
# local coherence: 연속된 두 문장간 응집도. 세 문장이므로 2개의 local coherence 있음
# global coherence: local coherence의 평균. 응집도 평균
LSAfun::coherence(DC$newdescp[77],
                  split='.',     # 문장구분자는 .
                  tvectors=TDMh)
